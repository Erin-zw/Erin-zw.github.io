<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Erin枫</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>来者犹可追</description>
    <pubDate>Sat, 14 Oct 2023 07:11:54 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>机翻数据集和Seq2Sseq</title>
      <link>http://example.com/2023/10/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E7%BF%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSeq2Sseq/</link>
      <guid>http://example.com/2023/10/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E7%BF%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSeq2Sseq/</guid>
      <pubDate>Sat, 14 Oct 2023 03:04:05 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;数据集&quot;&gt;&lt;a href=&quot;#数据集&quot; class=&quot;headerlink&quot; title=&quot;数据集&quot;&gt;&lt;/a&gt;数据集&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MTFraEng</span>(<span class="params">d2l.DataModule</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The English-French dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Defined in :numref:`sec_machine_translation`&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#下载数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_download</span>(<span class="params">self</span>):</span></span><br><span class="line">        d2l.extract(d2l.download(</span><br><span class="line">            d2l.DATA_URL+<span class="string">&#x27;fra-eng.zip&#x27;</span>, self.root,</span><br><span class="line">            <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.root + <span class="string">&#x27;/fra-eng/fra.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">return</span> f.read()</span><br><span class="line"><span class="comment">#预处理文本</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_preprocess</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`sec_machine_translation`&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Replace non-breaking space with space</span></span><br><span class="line">        text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="comment"># Insert space between words and punctuation marks</span></span><br><span class="line">        no_space = <span class="keyword">lambda</span> char, prev_char: char <span class="keyword">in</span> <span class="string">&#x27;,.!?&#x27;</span> <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line">        out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">               <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.lower())]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"><span class="comment">#将文本分成两部分，源文本和目标文本。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_tokenize</span>(<span class="params">self, text, max_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`sec_machine_translation`&quot;&quot;&quot;</span></span><br><span class="line">        src, tgt = [], []</span><br><span class="line">        <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">            <span class="keyword">if</span> max_examples <span class="keyword">and</span> i &gt; max_examples: <span class="keyword">break</span></span><br><span class="line">            parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># Skip empty tokens</span></span><br><span class="line">                src.append([t <span class="keyword">for</span> t <span class="keyword">in</span> <span class="string">f&#x27;<span class="subst">&#123;parts[<span class="number">0</span>]&#125;</span> &lt;eos&gt;&#x27;</span>.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">if</span> t])</span><br><span class="line">                tgt.append([t <span class="keyword">for</span> t <span class="keyword">in</span> <span class="string">f&#x27;<span class="subst">&#123;parts[<span class="number">1</span>]&#125;</span> &lt;eos&gt;&#x27;</span>.split(<span class="string">&#x27; &#x27;</span>) <span class="keyword">if</span> t])</span><br><span class="line">        <span class="keyword">return</span> src, tgt</span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_size, num_steps=<span class="number">9</span>, num_train=<span class="number">512</span>, num_val=<span class="number">128</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`sec_machine_translation`&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MTFraEng, self).__init__()</span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(</span><br><span class="line">            self._download())</span><br><span class="line"><span class="comment">#构造词表，生成源文本序列和目标文本序列（数字标记化）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_arrays</span>(<span class="params">self, raw_text, src_vocab=<span class="literal">None</span>, tgt_vocab=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`subsec_loading-seq-fixed-len`&quot;&quot;&quot;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_build_array</span>(<span class="params">sentences, vocab, is_tgt=<span class="literal">False</span></span>):</span></span><br><span class="line">            pad_or_trim = <span class="keyword">lambda</span> seq, t: (</span><br><span class="line">                seq[:t] <span class="keyword">if</span> <span class="built_in">len</span>(seq) &gt; t <span class="keyword">else</span> seq + [<span class="string">&#x27;&lt;pad&gt;&#x27;</span>] * (t - <span class="built_in">len</span>(seq)))</span><br><span class="line">            sentences = [pad_or_trim(s, self.num_steps) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">            <span class="keyword">if</span> is_tgt:</span><br><span class="line">                sentences = [[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>] + s <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line">            <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                vocab = d2l.Vocab(sentences, min_freq=<span class="number">2</span>)</span><br><span class="line">            array = d2l.tensor([vocab[s] <span class="keyword">for</span> s <span class="keyword">in</span> sentences])</span><br><span class="line">            valid_len = d2l.reduce_sum(</span><br><span class="line">                d2l.astype(array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>], d2l.int32), <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> array, vocab, valid_len</span><br><span class="line">        src, tgt = self._tokenize(self._preprocess(raw_text),</span><br><span class="line">                                  self.num_train + self.num_val)</span><br><span class="line">        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)</span><br><span class="line">        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, <span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> ((src_array, tgt_array[:,:-<span class="number">1</span>], src_valid_len, tgt_array[:,<span class="number">1</span>:]),</span><br><span class="line">                src_vocab, tgt_vocab)</span><br><span class="line"><span class="comment">#加载数据集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params">self, train</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`subsec_loading-seq-fixed-len`&quot;&quot;&quot;</span></span><br><span class="line">        idx = <span class="built_in">slice</span>(<span class="number">0</span>, self.num_train) <span class="keyword">if</span> train <span class="keyword">else</span> <span class="built_in">slice</span>(self.num_train, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">return</span> self.get_tensorloader(self.arrays, train, idx)</span><br><span class="line"><span class="comment">#构造</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, src_sentences, tgt_sentences</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Defined in :numref:`subsec_loading-seq-fixed-len`&quot;&quot;&quot;</span></span><br><span class="line">        raw_text = <span class="string">&#x27;\n&#x27;</span>.join([src + <span class="string">&#x27;\t&#x27;</span> + tgt <span class="keyword">for</span> src, tgt <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            src_sentences, tgt_sentences)])</span><br><span class="line">        arrays, _, _ = self._build_arrays(</span><br><span class="line">            raw_text, self.src_vocab, self.tgt_vocab)</span><br><span class="line">        <span class="keyword">return</span> arrays</span><br></pre></td></tr></table></figure><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqEncoder</span>(<span class="params">d2l.Encoder</span>):</span>  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The RNN encoder for sequence-to-sequence learning.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#初始化，传入词表尺寸，嵌入尺寸，隐藏单元数量，隐藏层数，drop概率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)<span class="comment">#嵌入层，根据词表的尺寸生成了一个维度为vocab_size*embed_size的向量表</span></span><br><span class="line">        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)<span class="comment">#使用GRU循环神经网络，进行编码</span></span><br><span class="line">        self.apply(init_seq2seq)<span class="comment">#应用初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, *args</span>):</span><span class="comment">#前向传播函数</span></span><br><span class="line">        <span class="comment"># X shape: (batch_size, num_steps)</span></span><br><span class="line">        embs = self.embedding(X.t().<span class="built_in">type</span>(torch.int64))<span class="comment">#生成嵌入向量</span></span><br><span class="line">        <span class="comment"># embs shape: (num_steps, batch_size, embed_size)</span></span><br><span class="line">        outputs, state = self.rnn(embs)<span class="comment">#GRU前向传播，生成outputs每个时间步最后的状态输出，state最后是时间步每个隐藏单元层的状态输出</span></span><br><span class="line">        <span class="comment"># outputs shape: (num_steps, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="comment"># state shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> outputs, state</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqDecoder</span>(<span class="params">d2l.Decoder</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The RNN decoder for sequence to sequence learning.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,</span><br><span class="line">                           num_layers, dropout)</span><br><span class="line">        self.dense = nn.LazyLinear(vocab_size)</span><br><span class="line">        self.apply(init_seq2seq)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_all_outputs, *args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> enc_all_outputs</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span><span class="comment">#输入tgt[:,:-1]，以及编码层输出，即outputs和state</span></span><br><span class="line">        <span class="comment"># X shape: (batch_size, num_steps)</span></span><br><span class="line">        <span class="comment"># embs shape: (num_steps, batch_size, embed_size)</span></span><br><span class="line">        embs = self.embedding(X.t().<span class="built_in">type</span>(torch.int32))<span class="comment">#嵌入转换</span></span><br><span class="line">        enc_output, hidden_state = state<span class="comment">#outputs最后一个时间步的最终输出状态作为context，最后一个时间步的所有隐藏单元状态作为下一个GRU的初始状态</span></span><br><span class="line">        <span class="comment"># context shape: (batch_size, num_hiddens)</span></span><br><span class="line">        context = enc_output[-<span class="number">1</span>]<span class="comment">#去最后一个时间步的状态输出作为上下文</span></span><br><span class="line">        <span class="comment"># Broadcast context to (num_steps, batch_size, num_hiddens)</span></span><br><span class="line">        context = context.repeat(embs.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)<span class="comment">#维度扩展</span></span><br><span class="line">        <span class="comment"># Concat at the feature dimension</span></span><br><span class="line">        embs_and_context = torch.cat((embs, context), -<span class="number">1</span>)<span class="comment">#连接上下文和输入变量</span></span><br><span class="line">        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)<span class="comment">#编码GRU前向传播</span></span><br><span class="line">        outputs = self.dense(outputs).swapaxes(<span class="number">0</span>, <span class="number">1</span>)<span class="comment">#全连接层将隐藏状态转换为尺寸为vocab_size的输出以便进行损失函数计算</span></span><br><span class="line">        <span class="comment"># outputs shape: (batch_size, num_steps, vocab_size)</span></span><br><span class="line">        <span class="comment"># hidden_state shape: (num_layers, batch_size, num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> outputs, [enc_output, hidden_state]</span><br></pre></td></tr></table></figure><h3 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">d2l.Classifier</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The base class for the encoder--decoder architecture.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Defined in :numref:`sec_encoder-decoder`&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><span class="comment">#前向传播函数，符合下面的图</span></span><br><span class="line">        enc_all_outputs = self.encoder(enc_X, *args)<span class="comment">#编码输出</span></span><br><span class="line">        dec_state = self.decoder.init_state(enc_all_outputs, *args)<span class="comment">#解码初始状态</span></span><br><span class="line">        <span class="comment"># Return decoder output only</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)[<span class="number">0</span>]<span class="comment">#解码输出</span></span><br></pre></td></tr></table></figure><p><img src="/images/深度学习/s2s/1.svg" style="zoom:67%;"></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">data = d2l.MTFraEng(batch_size=<span class="number">128</span>)<span class="comment">#数据集</span></span><br><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">256</span>, <span class="number">256</span>, <span class="number">2</span>, <span class="number">0.2</span><span class="comment">#参数</span></span><br><span class="line">encoder = Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)<span class="comment">#编码器</span></span><br><span class="line">decoder = Seq2SeqDecoder(</span><br><span class="line">    <span class="built_in">len</span>(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)<span class="comment">#解码器</span></span><br><span class="line">model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>],<span class="comment">#模型</span></span><br><span class="line">                lr=<span class="number">0.005</span>)</span><br><span class="line">trainer = d2l.Trainer(max_epochs=<span class="number">30</span>, gradient_clip_val=<span class="number">1</span>, num_gpus=<span class="number">1</span>)</span><br><span class="line">trainer.fit(model, data)</span><br><span class="line"></span><br><span class="line">d2l.plt.show()</span><br></pre></td></tr></table></figure><p><strong>训练分析：</strong></p><p>解码器输出是一个形状为(batch_size, num_steps, vocab_size)的输出，这个输出和形状为(batch_size, num_steps, 1)的target进行交叉熵损失函数计算损失，然后进行梯度反向传播优化模型</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E7%BF%BB%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8CSeq2Sseq/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RNN</title>
      <link>http://example.com/2023/10/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/</link>
      <guid>http://example.com/2023/10/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/</guid>
      <pubDate>Mon, 09 Oct 2023 09:06:59 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;原理流程&quot;&gt;&lt;a href=&quot;#原理流程&quot; class=&quot;headerlink&quot; title=&quot;原理流程&quot;&gt;&lt;/a&gt;原理流程&lt;/h2&gt;&lt;p&gt;这篇文章我们从后向前分析：&lt;/p&gt;
&lt;p&gt;以下是训练过程：&lt;/p&gt;
&lt;figure class=&quot;highlight pyt</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="原理流程"><a href="#原理流程" class="headerlink" title="原理流程"></a>原理流程</h2><p>这篇文章我们从后向前分析：</p><p>以下是训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = d2l.TimeMachine(batch_size=<span class="number">1024</span>, num_steps=<span class="number">32</span>)<span class="comment">#数据集</span></span><br><span class="line">rnn = RNNScratch(num_inputs=<span class="built_in">len</span>(data.vocab), num_hiddens=<span class="number">32</span>)<span class="comment">#rnn模型</span></span><br><span class="line">model = RNNLMScratch(rnn, vocab_size=<span class="built_in">len</span>(data.vocab), lr=<span class="number">1</span>)<span class="comment">#语言模型</span></span><br><span class="line">trainer = d2l.Trainer(max_epochs=<span class="number">100</span>, gradient_clip_val=<span class="number">1</span>, num_gpus=<span class="number">1</span>)<span class="comment">#训练器</span></span><br><span class="line">trainer.fit(model, data)<span class="comment">#训练</span></span><br></pre></td></tr></table></figure><p><strong>1.</strong>在训练过程中，我们首先调用了 d2l.TimeMachine数据集，这个数据集是我们自建的，简要建立过程如下：每一个X代表了一个字母。输入表的行数就是num_steps，列数就是数据集大小。并且数据集还将出现的字母和空格还有未知编成了一个vocab表以便索引和编码，所以实际上X是不同的数字，范围从0-27.</p><table>    <tr>        <td colspan="3">输入</td><td colspan="3">输出</td>    </tr>    <tr>        <th>x[0]</th><th>x[1]</th><th>x[2]</th><th>x[1]</th><th>x[2]</th><th>x[3]</th>    </tr>    <tr>        <th>x[1]</th><th>x[2]</th><th>x[3]</th><th>x[2]</th><th>x[3]</th><th>x[4]</th>    </tr>    <tr>        <th>x[2]</th><th>x[3]</th><th>x[4]</th><th>x[3]</th><th>x[4]</th><th>x[5]</th>    </tr>    <tr>        <td colspan="6">....</td>    </tr></table><p><strong>2.</strong>紧接着初始化了rnn模型，初始化参数为输入数和隐藏单元数量。</p><p>RNN的前向传播函数如下所示，在for循环中生成了隐藏层的输出并用state临时保存，再存入outputs以便后续使用。outputs形状为n*b*h</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, state=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Initial state with shape: (batch_size, num_hiddens)</span></span><br><span class="line">        state = torch.zeros((inputs.shape[<span class="number">1</span>], self.num_hiddens),</span><br><span class="line">                          device=inputs.device)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        state, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:  <span class="comment"># Shape of inputs: (num_steps, batch_size, num_inputs)</span></span><br><span class="line">        state = torch.tanh(torch.matmul(X, self.W_xh) +</span><br><span class="line">                         torch.matmul(state, self.W_hh) + self.b_h)</span><br><span class="line">        outputs.append(state)</span><br><span class="line">    <span class="keyword">return</span> outputs, state</span><br></pre></td></tr></table></figure><p><img src="\images\深度学习\RNN\1.png" style="zoom:67%;"></p><p><img src="\images\深度学习\RNN\2.png" style="zoom:67%;"></p><p>这个时候再来看我们的模型就会很清晰了，实际上模型图上的每一个隐藏单元组（图中的方框），是同一个隐藏单元组，只不过图示为了清晰表示隐藏单元的循环继承性画成这样。</p><p><strong>3.</strong>然后初始化了语言模型，语言模型传入了rnn模型，vocab表长度，训练步长的超参数。</p><p><strong>4.</strong>然后我们直接看trainer.fit，这个函数会先调用RNNLMScratch模型的train_step函数，然后train_step函数会调用交叉熵损失函数，交叉熵损失函数有两个参数，一个为预测输出，来自RNNLMScratch模型的前向传播函数，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defined in :numref:`sec_rnn-scratch`&quot;&quot;&quot;</span></span><br><span class="line">    embs = self.one_hot(X)</span><br><span class="line">    rnn_outputs, _ = self.rnn(embs, state)</span><br><span class="line">    <span class="keyword">return</span> self.output_layer(rnn_outputs)</span><br></pre></td></tr></table></figure><p>他先将输入进行独热编码，并且独热编码长度为vocab的长度，也就是每一个vocab中的元素可用一个类似于分类器输出的编码来表示。进行独热编码后，X形状变成num_steps*batch_size*vocab_size(X在one_hot函数我们将它转置以便rnn的前向传播函数方便),然后再将编码后的X送入rnn的前向传播函数，最后再将rnn输出保存的不同时间步的隐藏单元状态输入给输出函数output_layer即可。输出函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">output_layer</span>(<span class="params">self, rnn_outputs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defined in :numref:`sec_rnn-scratch`&quot;&quot;&quot;</span></span><br><span class="line">    outputs = [d2l.matmul(H, self.W_hq) + self.b_q <span class="keyword">for</span> H <span class="keyword">in</span> rnn_outputs]</span><br><span class="line">    <span class="keyword">return</span> d2l.stack(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>我们可以看到，outputs为一个张量的列表，列表中每一个元素形状为batch_size*vocab_size，列表长度为num_steps，我们用stack函数在dim1上进行连结，最终输出形状为batch_size*num_steps*vocab_size。这就是损失函数第一个参数预测输出的来源。</p><p>损失函数第二个参数就是实际输出，实际输出形状我们知道是batch_size*num_steps，然后我们就可以利用交叉熵损失函数，进行一个损失计算。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RNN/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>序列模型</title>
      <link>http://example.com/2023/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</link>
      <guid>http://example.com/2023/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</guid>
      <pubDate>Thu, 05 Oct 2023 10:27:01 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
x_t～P(x_t|x_{t-1},x_{t-2},...,x_{t-\tau})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="原理">原理</h2><p><span class="math display">\[x_t～P(x_t|x_{t-1},x_{t-2},...,x_{t-\tau})\]</span></p><p><span class="math inline">\(x_t\)</span>处的值只与他前面<span class="math inline">\(\tau\)</span>个输入有关。</p><h2 id="训练">训练</h2><p>选取1000个时间步，使用正弦函数和一些可加性噪声来生成序列数据。</p><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>(<span class="params">d2l.DataModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_size=<span class="number">16</span>, T=<span class="number">1000</span>, num_train=<span class="number">600</span>, tau=<span class="number">4</span></span>):</span></span><br><span class="line">        self.save_hyperparameters()</span><br><span class="line">        self.time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">        self.x = torch.sin(<span class="number">0.01</span> * self.time) + torch.randn(T) * <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">data = Data()</span><br><span class="line">d2l.plot(data.time, data.x, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>数据集加载器，特征选取<span class="math inline">\(\tau\)</span>个作为输入，标签选择第<span class="math inline">\(\tau+1\)</span>个值。此处选择4个为一组训练其后的数据。所以实际上数据集只有996个。</p><p>并且我们也规定了训练集数据只有600个。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@d2l.add_to_class(<span class="params">Data</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params">self, train</span>):</span></span><br><span class="line">    features = [self.x[i : self.T-self.tau+i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.tau)]</span><br><span class="line">    self.features = torch.stack(features, <span class="number">1</span>)</span><br><span class="line">    self.labels = self.x[self.tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    i = <span class="built_in">slice</span>(<span class="number">0</span>, self.num_train) <span class="keyword">if</span> train <span class="keyword">else</span> <span class="built_in">slice</span>(self.num_train, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">return</span> self.get_tensorloader([self.features, self.labels], train, i)</span><br></pre></td></tr></table></figure><p>数据集表格：</p><table><tr><td colspan="4">输入</td><td>输出</td></tr><tr><th>x[0]</th><th>x[1]</th><th>x[2]</th><th>x[3(tau-1)]</th><th>x[4]</th></tr><tr><th>x[1]</th><th>x[2]</th><th>x[3]</th><th>x[4]</th><th>x[5]</th></tr><tr><th>x[2]</th><th>x[3]</th><th>x[4]</th><th>x[5]</th><th>x[6]</th></tr><tr><td colspan="5">....</td></tr><tr><th>x[995]</th><th>x[996]</th><th>x[997]</th><th>x[998]</th><th>x[999]</th></tr></table><p>此处我们选择线性模型用来训练模型，训练完成后检测模型在预测中的表现拟合的非常好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = model(data.features).detach().numpy()</span><br><span class="line">d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>但是此处我们的预测相当于已知所有输入再喂给模型，模型当然可以很好的拟合。但是序列模型是需要预测的，也就是说根据训练集实际上我们只能拟合到x[605]处的值。后面的需要一步一步进行预测拟合。<span class="math display">\[\begin{split}\begin{aligned}\hat{x}_{605} &amp;= f(x_{601}, x_{602}, x_{603}, x_{604}), \\\hat{x}_{606} &amp;= f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\\hat{x}_{607} &amp;= f(x_{603}, x_{604}, \hat{x}_{605},\hat{x}_{606}),\\\hat{x}_{608} &amp;= f(x_{604}, \hat{x}_{605}, \hat{x}_{606},\hat{x}_{607}),\\\hat{x}_{609} &amp;= f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607},\hat{x}_{608}),\\&amp;\vdots\end{aligned}\end{split}\]</span>代码实现后我们发现经过几步之后，预测很快就会衰减到一个常数。这归因于错误的累积。也就是说每一步的误差都会累积从而导致后续出现比较大的误差。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>N-W核回归</title>
      <link>http://example.com/2023/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NW%E6%A0%B8%E5%9B%9E%E5%BD%92/</link>
      <guid>http://example.com/2023/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NW%E6%A0%B8%E5%9B%9E%E5%BD%92/</guid>
      <pubDate>Wed, 04 Oct 2023 06:16:38 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\images\深度学习\attention\1.png&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;\images\深度学习\attention\2.png&quot; style=&quot;zo</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="原理">原理</h2><p><img src="\images\深度学习\attention\1.png" style="zoom:67%;"></p><p><img src="\images\深度学习\attention\2.png" style="zoom:67%;"></p><p>在这里，对于key，value，query可以这么理解，key相当于非自主提示，例如咖啡杯是红的十分显眼，他的key对应的value非常大，value相当于是不同key不同非自主提示所对应的一个输入值，而query在这里充当了一个自主提示的作用，和key键值通过某种映射产生不同的权值。因为我们知道注意力汇聚的公式如下：α相当于就是一种映射规则。<span class="math display">\[f(\mathbf{q}) = \sum_i \mathbf{v}_i \frac{\alpha(\mathbf{q},\mathbf{k}_i)}{\sum_j \alpha(\mathbf{q}, \mathbf{k}_j)}.\]</span> 正是因为有这样的query将注意力机制和全连接层进行了区分。</p><p>经过学习，我们知道α是核函数： <span class="math display">\[\begin{split}\begin{aligned}\alpha(\mathbf{q}, \mathbf{k}) &amp; = \exp\left(-\frac{1}{2}\|\mathbf{q} - \mathbf{k}\|^2 \right) &amp;&amp; \textrm{Gaussian;} \\\alpha(\mathbf{q}, \mathbf{k}) &amp; = 1 \textrm{ if } \|\mathbf{q} -\mathbf{k}\| \leq 1 &amp;&amp; \textrm{Boxcar;} \\\alpha(\mathbf{q}, \mathbf{k}) &amp; = \mathop{\mathrm{max}}\left(0, 1 -\|\mathbf{q} - \mathbf{k}\|\right) &amp;&amp; \textrm{Epanechikov.}\end{aligned}\end{split}\]</span> 常见核函数图像：</p><p><img src="\images\深度学习\attention\3.png" style="zoom:67%;"></p><p><img src="\images\深度学习\attention\4.png" style="zoom:67%;"></p><p>内核函数理解为query和key之间的距离，这些距离是内核函数的输入，并且内核函数在query=key也即输入为0达到最大值，这很正常，因为query等于key就说明这个key所对应value占比重很大。故也就可以用注意力汇聚实现N-W核回归。</p><p>“The attention weight is assigned according to the similarity (ordistance) between query and key”很好地解释了。</p><p>也就是说我们要拟合一个函数，可以将x_val，y_val作为一个已知数据，x_val是键值key，我们再定义x_train作为query（x_val和x_train范围一致，分布不同），而根据函数再给予y_train一定的扰动作为噪音，这样我们将x_train*attention_weight就可以得到我们的注意力汇集输出y_hat，我们再将y_hat与y_val放在同一图像进行对比即可观察拟合情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x</span><br><span class="line">n = <span class="number">40</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n) * <span class="number">5</span>)</span><br><span class="line">y_train = f(x_train) + torch.randn(n)</span><br><span class="line">x_val = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">y_val = f(x_val)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nadaraya_watson</span>(<span class="params">x_train, y_train, x_val, kernel</span>):</span></span><br><span class="line">    dists = x_train.reshape((-<span class="number">1</span>, <span class="number">1</span>)) - x_val.reshape((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Each column/row corresponds to each query/key</span></span><br><span class="line">    k = kernel(dists).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">    <span class="comment"># Normalization over keys for each query</span></span><br><span class="line">    attention_w = k / k.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br><span class="line">    y_hat = y_train@attention_w</span><br><span class="line">    <span class="keyword">return</span> y_hat, attention_w</span><br><span class="line"><span class="comment">#我们要展示的对比</span></span><br><span class="line">ax.plot(x_val, y_hat)</span><br><span class="line">ax.plot(x_val, y_val, <span class="string">&#x27;m--&#x27;</span>)</span><br><span class="line">ax.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>);</span><br></pre></td></tr></table></figure><p>我们再进行观察，例如上侧的高斯核函数，我们知道实际上的高斯函数是这样的：<span class="math display">\[\begin{split}\begin{aligned}\alpha(\mathbf{q}, \mathbf{k}) &amp; = \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{q} - \mathbf{k}\|^2 \right) &amp;&amp; \textrm{Gaussian;}\\\end{aligned}\end{split}\]</span> 我们知道随着<span class="math inline">\(\sigma\)</span>的增大，高斯函数会变窄，也就是说我们可以通过调节<span class="math inline">\(\sigma\)</span>的值就可以调整高斯核函数的形状，也就可以改变注意力矩阵权重，从而通过调整对函数达到理想的拟合。</p><p>关于注意力机制没学RNN还是有难度，先学RNN去了（</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NW%E6%A0%B8%E5%9B%9E%E5%BD%92/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>reshape逻辑</title>
      <link>http://example.com/2023/10/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/reshape%E9%80%BB%E8%BE%91/</link>
      <guid>http://example.com/2023/10/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/reshape%E9%80%BB%E8%BE%91/</guid>
      <pubDate>Tue, 03 Oct 2023 13:06:39 GMT</pubDate>
      
        
        
      <description>&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</description>
        
      
      
      
      <content:encoded><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">4</span>).reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[[[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">0.</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">          [<span class="number">0.</span>, <span class="number">1.</span>]]]])</span><br></pre></td></tr></table></figure><p>优先选取行，取完一行再取下一行。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/reshape%E9%80%BB%E8%BE%91/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>分类网络原理初步</title>
      <link>http://example.com/2023/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E5%88%9D%E6%AD%A5/</link>
      <guid>http://example.com/2023/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E5%88%9D%E6%AD%A5/</guid>
      <pubDate>Sun, 01 Oct 2023 09:14:25 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;lenet&quot;&gt;Lenet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/深度学习/class/4.png&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;alexnet&quot;&gt;Alexnet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/image</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="lenet">Lenet</h2><p><img src="/images/深度学习/class/4.png" style="zoom:67%;"></p><h2 id="alexnet">Alexnet</h2><p><img src="/images/深度学习/class/5.png" style="zoom:67%;"></p><h2 id="vgg">VGG</h2><p><img src="/images/深度学习/class/1.png" style="zoom:67%;"></p><h2 id="nin">NiN</h2><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。<em>网络中的网络</em>（<em>NiN</em>）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机</p><p><img src="/images/深度学习/class/6.png" style="zoom:67%;"></p><h2 id="googlenet">Googlenet</h2><p><img src="/images/深度学习/class/7.png" style="zoom:67%;"></p><p><img src="/images/深度学习/class/8.png" style="zoom:67%;"></p><h2 id="resnet">Resnet</h2><p>在He的文章中已经说过，梯度爆炸和梯度消失已经很大程度上被批量规范化给解决了，而深度网络在训练集上的训练错误比浅层网络还大，说明不是出现了过拟合。而是因为深度网络存在退化问题，也就是说</p><p>看了很多博客，现在我的理解就是Resnet给了深度网络试错的能力，从而有效防止了深层网络的退化问题。</p><p>退化，按照大部分博客理解就是非线性的激活函数的存在，会造成很多不可逆的信息损失。网络加深到一定程度，过多的信息损失就会造成网络的退化。</p><p>所谓试错，可以这么理解，深层网络可以看成一个浅层网络加上后面的层，这个浅层网络本身已经是一个较优解但还够优，所以我们加上残差块，而残差块H(X)=F(X)+X,在训练中，如果网络训练中发现出现了退化，网络会倾向于把F(X)训练为0，也就是H(X)=X也即恒等映射。而如果残差块把较优解更优了，那么F(X)就不用被训练为0，相当于提供了试错。同时H(X)=F(X)+X也有效防止了梯度消失，反向传播加了1梯度。</p><p>也有人理解成相当于给了神经网络自己选择自己网络结构的一种机会通过shortcut这种连接。</p><p>同时在输出的Relu激活端，由于shortcut的X，信息冗余度提高，可以有效防止激活函数丢失有效信息。</p><p>1×1卷积让输入输出形状相同。</p><p><img src="/images/深度学习/class/3.png" style="zoom:67%;"></p><p><img src="/images/深度学习/class/2.png" style="zoom:67%;"></p><h2 id="densenet">Densenet</h2>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%E5%88%9D%E6%AD%A5/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Resnet</title>
      <link>http://example.com/2023/09/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Resnet/</link>
      <guid>http://example.com/2023/09/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Resnet/</guid>
      <pubDate>Sat, 30 Sep 2023 01:36:32 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;Resnet在干什么&quot;&gt;&lt;a href=&quot;#Resnet在干什么&quot; class=&quot;headerlink&quot; title=&quot;Resnet在干什么&quot;&gt;&lt;/a&gt;Resnet在干什么&lt;/h2&gt;&lt;p&gt;在He的文章中已经说过，梯度爆炸和梯度消失已经很大程度上被批量规范化给解决了</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="Resnet在干什么"><a href="#Resnet在干什么" class="headerlink" title="Resnet在干什么"></a>Resnet在干什么</h2><p>在He的文章中已经说过，梯度爆炸和梯度消失已经很大程度上被批量规范化给解决了，而深度网络在训练集上的训练错误比浅层网络还大，说明不是出现了过拟合。而是因为深度网络存在退化问题，也就是说</p><p>看了很多博客，现在我的理解就是Resnet给了深度网络试错的能力，从而有效防止了深层网络的退化问题。</p><p>退化，按照大部分博客理解就是非线性的激活函数的存在，会造成很多不可逆的信息损失。网络加深到一定程度，过多的信息损失就会造成网络的退化。</p><p>所谓试错，可以这么理解，深层网络可以看成一个浅层网络加上后面的层，这个浅层网络本身已经是一个较优解但还够优，所以我们加上残差块，而残差块H(X)=F(X)+X,在训练中，如果网络训练中发现出现了退化，网络会倾向于把F(X)训练为0，也就是H(X)=X也即恒等映射。而如果残差块把较优解更优了，那么F(X)就不用被训练为0，相当于提供了试错。同时H(X)=F(X)+X也有效防止了梯度消失，反向传播加了1梯度。</p><p>也有人理解成相当于给了神经网络自己选择自己网络结构的一种机会通过shortcut这种连接。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/09/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Resnet/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>二维互相关预算</title>
      <link>http://example.com/2023/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%8C%E7%BB%B4%E4%BA%92%E7%9B%B8%E5%85%B3/</link>
      <guid>http://example.com/2023/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%8C%E7%BB%B4%E4%BA%92%E7%9B%B8%E5%85%B3/</guid>
      <pubDate>Sun, 24 Sep 2023 12:34:56 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;图像卷积&quot;&gt;&lt;a href=&quot;#图像卷积&quot; class=&quot;headerlink&quot; title=&quot;图像卷积&quot;&gt;&lt;/a&gt;图像卷积&lt;/h2&gt;&lt;p&gt;在深度学习中，卷积一般是二维互相关运算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/深度学习/二维互相关/1.png</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h2><p>在深度学习中，卷积一般是二维互相关运算。</p><p><img src="/images/深度学习/二维互相关/1.png" style="zoom:67%;"></p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><h3 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h3><p>我们使用一个pytorch附带的函数作为例子：<br><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p><p>下面是一个padding=1（实际上行列填充为2），垂直stride为3，水平stride为2的卷积运算。</p><p>代码为：<code>torch.nn.Conv2d(1, 1, 2, (2,3), 1)</code></p><p><img src="/images/深度学习/二维互相关/2.png" style="zoom:67%;"></p><p>输出形状计算公式：</p><script type="math/tex; mode=display">\frac{n_h-k_h+p_h+s_h}{s_h}×\frac{n_w-k_w+p_w+s_w}{s_w}\\=\frac{3-2+2+2}{2}×\frac{3-2+2+3}{3}\\=2.5×2\\=2×2</script><p>一般来讲在pytorch中，如果遇到输出形状有小数，舍去为整数（不是四舍五入）。</p><p>如果padding参数为(1,0)这样的元组，代表在第一个维度也即行上添加了两行0元素，第二个维度也即列上没有填充。</p><h3 id="多输入多输出"><a href="#多输入多输出" class="headerlink" title="多输入多输出"></a>多输入多输出</h3><p>我们使用一个pytorch附带的函数作为例子：<br><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p><p>in_channels代表输入通道数，类似灰白图像一个通道，RGB图像三个通道。</p><p>out_channels代表输出通道数，通俗来讲就是卷积核的个数。</p><p>且多输入和多输出通道卷积要求输入通道数等于每个卷积核通道数。</p><p>也就是说在这个函数中，你只需要设置好in_channels参数确定好输入通道数，kernel_size卷积核每个通道尺寸，框架会自动设置每个卷积核通道个数。</p><p>总结来说就是：</p><ul><li>输出通道数=卷积核个数=out_channels参数</li><li>输入通道数=每个卷积核的通道数=in_channels参数</li></ul><p>下面看一个例子：<br><img src="/images/深度学习/二维互相关/3.png" style="zoom:67%;"></p><p>实际上在这里，in_channels=3，out_channels=2，kernel_size=1,卷积核通道数自动设置为3.</p><p>并且这边也是一个1<em>1卷积，实际上是对图像不同通道的同一位置像素的一个权重线性和，类似于全连接。使用1\</em>1卷积可以关注到每个输入通道之间的关系，更容易捕捉到特征。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/09/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BA%8C%E7%BB%B4%E4%BA%92%E7%9B%B8%E5%85%B3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>卷积</title>
      <link>http://example.com/2023/09/23/%E4%BF%A1%E5%8F%B7/%E5%8D%B7%E7%A7%AF/</link>
      <guid>http://example.com/2023/09/23/%E4%BF%A1%E5%8F%B7/%E5%8D%B7%E7%A7%AF/</guid>
      <pubDate>Sat, 23 Sep 2023 11:57:08 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;拿一维卷积举个例子：&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
假设x(t)为输入,h(t)为系统响应\\
x(t)*h(t)=\int{x(a)h(t-a)}\\
\delta(t)*h(t)=h(t)\\
\delta(t-k\</description>
        
      
      
      
      <content:encoded><![CDATA[<p>拿一维卷积举个例子：</p><p><span class="math display">\[假设x(t)为输入,h(t)为系统响应\\x(t)*h(t)=\int{x(a)h(t-a)}\\\delta(t)*h(t)=h(t)\\\delta(t-k\Delta)*h(t)=h(t-k\Delta)\\x(t)= \sum_{k=-\infty}^{\infty}{x(k\Delta)\delta(t-k\Delta)\Delta}\\当x(t)输入，输入x(t)是这些冲击信号的叠加，每一个\delta(t-k\Delta)都是\delta(t)的平移，并且会对h(t)进行响应，\\那么输出也是这些冲激信号对于系统响应的叠加，即：\\y(t)=\sum_{k=-\infty}^{\infty}{x(k\Delta)h(t-k\Delta)\Delta}\]</span>所以也就可以看成是输入在每一个点的值是一个系数或者权重，这些系数分别乘上系统响应再求和就是输入了。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E4%BF%A1%E5%8F%B7/">信号</category>
      
      
      
      <comments>http://example.com/2023/09/23/%E4%BF%A1%E5%8F%B7/%E5%8D%B7%E7%A7%AF/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>pytorch中dim=0，1求mean，sum</title>
      <link>http://example.com/2023/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/dim/</link>
      <guid>http://example.com/2023/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/dim/</guid>
      <pubDate>Sat, 23 Sep 2023 08:09:49 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h2&gt;&lt;p&gt;dim=（1，2，3）也就是说把这三个维度求平均，保留dim=0维度不变。故求平均后其他维度变成1，dim=0仍是2.&lt;/p&gt;
&lt;figur</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>dim=（1，2，3）也就是说把这三个维度求平均，保留dim=0维度不变。故求平均后其他维度变成1，dim=0仍是2.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>,dtype = <span class="built_in">float</span>).reshape((<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line">mean = X.mean(dim=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(mean,mean.shape)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">          [ <span class="number">2.</span>,  <span class="number">3.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">          [ <span class="number">6.</span>,  <span class="number">7.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">10.</span>, <span class="number">11.</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">12.</span>, <span class="number">13.</span>],</span><br><span class="line">          [<span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">16.</span>, <span class="number">17.</span>],</span><br><span class="line">          [<span class="number">18.</span>, <span class="number">19.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">20.</span>, <span class="number">21.</span>],</span><br><span class="line">          [<span class="number">22.</span>, <span class="number">23.</span>]]]], dtype=torch.float64)</span><br><span class="line">tensor([[[[ <span class="number">5.5000</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">17.5000</span>]]]], dtype=torch.float64) torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>下面看dim=（0，2，3）的输出，也就是说保留通道数不变，其他维度求平均变为1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">          [ <span class="number">2.</span>,  <span class="number">3.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">          [ <span class="number">6.</span>,  <span class="number">7.</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">          [<span class="number">10.</span>, <span class="number">11.</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">12.</span>, <span class="number">13.</span>],</span><br><span class="line">          [<span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">16.</span>, <span class="number">17.</span>],</span><br><span class="line">          [<span class="number">18.</span>, <span class="number">19.</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">20.</span>, <span class="number">21.</span>],</span><br><span class="line">          [<span class="number">22.</span>, <span class="number">23.</span>]]]], dtype=torch.float64)</span><br><span class="line">tensor([[[[ <span class="number">7.5000</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">11.5000</span>]],</span><br><span class="line"></span><br><span class="line">         [[<span class="number">15.5000</span>]]]], dtype=torch.float64) torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      
      <comments>http://example.com/2023/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/dim/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
